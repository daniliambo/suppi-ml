{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "simple_cols = ['BEDCERT', 'RESTOT', 'INHOSP', 'CCRC_FACIL', 'SFF', 'CHOW_LAST_12MOS', 'SPRINKLER_STATUS', 'EXP_TOTAL', 'ADJ_TOTAL']\n",
    "\n",
    "class ColumnSelectTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        return X[self.columns].values\n",
    "        \n",
    "simple_features = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(simple_cols)),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "simple_features_model = Pipeline([\n",
    "    ('simple', simple_features),\n",
    "    ('linear', LinearRegression()),\n",
    "])\n",
    "\n",
    "simple_features_model.fit(data, fine_counts > 0)\n",
    "\n",
    "Pipeline(memory=None,\n",
    "         steps=[('simple',\n",
    "                 Pipeline(memory=None,\n",
    "                          steps=[('cst',\n",
    "                                  ColumnSelectTransformer(columns=['BEDCERT',\n",
    "                                                                   'RESTOT',\n",
    "                                                                   'INHOSP',\n",
    "                                                                   'CCRC_FACIL',\n",
    "                                                                   'SFF',\n",
    "                                                                   'CHOW_LAST_12MOS',\n",
    "                                                                   'SPRINKLER_STATUS',\n",
    "                                                                   'EXP_TOTAL',\n",
    "                                                                   'ADJ_TOTAL'])),\n",
    "                                 ('imputer',\n",
    "                                  SimpleImputer(add_indicator=False, copy=True,\n",
    "                                                fill_value=None,\n",
    "                                                missing_values=nan,\n",
    "                                                strategy='mean', verbose=0))],\n",
    "                                                \n",
    "                          verbose=False)),\n",
    "                ('linear',\n",
    "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
    "                                  normalize=False))],\n",
    "         verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='mean') # Your code here\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "]) # Your code here\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor() # Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep dive into sklearn.pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Deep Dive Into Sklearn Pipelines -> article\n",
    "\n",
    "\n",
    "\n",
    "- pipelines for repetitive experiments\n",
    "- Pipelines are set up with the fit/transform/predict functionality, so you can fit a whole pipeline to the training data and transform to the test data, without having to do it individually for each thing you do.\n",
    "- 2 types of transformations: dependent on a data and not\n",
    "- split to numberic features / object features\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# select single cols\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process single cols\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='processed')),\n",
    "                ('tfidf', TfidfVectorizer( stop_words='english'))\n",
    "            ])\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas)])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "preds = pipeline.predict(X_test)\n",
    "np.mean(preds == y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch for pipelines\n",
    "\n",
    "pipeline.get_params().keys()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = { 'features__text__tfidf__max_df': [0.9, 0.95],\n",
    "                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "                   'classifier__max_depth': [50, 70],\n",
    "                    'classifier__min_samples_leaf': [1,2]\n",
    "                  }\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#refitting on entire training data using best settings\n",
    "clf.refit\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)\n",
    "\n",
    "np.mean(preds == y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer\n",
    "\n",
    "# \"This estimator allows different columns or column subsets\n",
    "# of the input to be transformed separately and the features\n",
    "# generated by each transformer will be concatenated to form a single feature space. \n",
    "\n",
    "# This is useful for heterogeneous or columnar data, to combine several feature extraction\n",
    "#     mechanisms or transformations into a single transformer.\"\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale',MinMaxScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one-hot',OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col_trans = ColumnTransformer(transformers=[\n",
    "    ('num_pipeline',num_pipeline,num_cols),\n",
    "    ('cat_pipeline',cat_pipeline,cat_cols)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create ColumnTransformer to Apply the Pipeline for Each Column Set\n",
    "#  ColumnTransformer(transformers=[(‘step name’, transform function,cols), …])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Optional, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseDataPreprocessor(TransformerMixin):\n",
    "    def __init__(self, needed_columns: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        :param needed_columns: if not None select these columns from the dataframe\n",
    "        \"\"\"\n",
    "        self.needed_columns = needed_columns\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, data, *args):\n",
    "        \"\"\"\n",
    "        Prepares the class for future transformations\n",
    "        :param data: pd.DataFrame with all available columns\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.needed_columns:\n",
    "            data = data.loc[:, self.needed_columns]\n",
    "\n",
    "        self.scaler.fit(X=data)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> np.array:\n",
    "        \n",
    "        \"\"\"\n",
    "        Transforms features so that they can be fed into the regressors\n",
    "        :param data: pd.DataFrame with all available columns\n",
    "        :return: np.array with preprocessed features\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.needed_columns:\n",
    "            data = data.loc[:, self.needed_columns]\n",
    "        return self.scaler.transform(X=data)\n",
    "    \n",
    "preprocessor = BaseDataPreprocessor(needed_columns=continuous_columns)\n",
    "\n",
    "X_train = preprocessor.fit_transform(data_train)\n",
    "X_test = preprocessor.transform(data_test)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "class SmartDataPreprocessor(TransformerMixin):\n",
    "    def __init__(self, needed_columns: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        :param needed_columns: if not None select these columns from the dataframe\n",
    "        \"\"\"\n",
    "        self.needed_columns = needed_columns\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def add_features(self, data: pd.DataFrame):\n",
    "        cols = data.columns\n",
    "        if 'Lot_Frontage' in cols and 'Lot_Area' in cols:\n",
    "            data['Generated_Feature'] = data['Lot_Frontage'] + data['Lot_Area']\n",
    "        return data\n",
    "\n",
    "    def fit(self, data, *args):\n",
    "        \"\"\"\n",
    "        Prepares the class for future transformations\n",
    "        :param data: pd.DataFrame with all available columns\n",
    "        :return: self\n",
    "\n",
    "        Feature Generation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.needed_columns:\n",
    "            data = data.loc[:, self.needed_columns]\n",
    "\n",
    "        data = self.add_features(data)\n",
    "        self.scaler.fit(X=data)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> np.array:\n",
    "        \"\"\"\n",
    "        Transforms features so that they can be fed into the regressors\n",
    "        :param data: pd.DataFrame with all available columns\n",
    "        :return: np.array with preprocessed features\n",
    "\n",
    "        Feature Generation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.needed_columns:\n",
    "            data = data.loc[:, self.needed_columns]\n",
    "\n",
    "        data = self.add_features(data)\n",
    "\n",
    "        return self.scaler.transform(X=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, List\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import sklearn.base\n",
    "\n",
    "\n",
    "def replace_zero(data):\n",
    "    mask = (data == 0)\n",
    "    nonzero_mean = np.mean(data[(1 - mask).astype(bool)])\n",
    "    dc = data.copy()\n",
    "    dc[mask] = nonzero_mean\n",
    "    return dc\n",
    "\n",
    "\n",
    "class BaseDataPreprocessor(TransformerMixin):\n",
    "    def __init__(self, needed_columns: Optional[List[str]]=None):\n",
    "        \"\"\"\n",
    "        :param needed_columns: if not None select these columns from the dataframe\n",
    "        \"\"\"\n",
    "        self.scaler = StandardScaler()\n",
    "        self.needed_columns = needed_columns\n",
    "\n",
    "    def fit(self, data, *args):\n",
    "        \"\"\"\n",
    "        Prepares the class for future transformations\n",
    "        :param data: pd.DataFrame with all available columns\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        X = data\n",
    "        if self.needed_columns is not None:\n",
    "            X = data[self.needed_columns]\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> np.array:\n",
    "        \"\"\"\n",
    "        Transforms features so that they can be fed into the regressors\n",
    "        :param data: pd.DataFrame with all available columns\n",
    "        :return: np.array with preprocessed features\n",
    "        \"\"\"\n",
    "        X = data\n",
    "        if self.needed_columns is not None:\n",
    "            X = data[self.needed_columns]\n",
    "        return np.array(self.scaler.transform(X))\n",
    "\n",
    "class OneHotPreprocessor(BaseDataPreprocessor):\n",
    "    def __init__(self, cat_cols, gaps, **kwargs):\n",
    "        super(OneHotPreprocessor, self).__init__(**kwargs)\n",
    "        self.enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.city_center = (42.025333, -93.633801)\n",
    "        self.cols = cat_cols\n",
    "        self.gaps = gaps\n",
    "\n",
    "    def fit(self, data, *args):\n",
    "        d = data.copy()\n",
    "        if self.gaps is not None:\n",
    "            for g in self.gaps:\n",
    "                d[g] = replace_zero(d[g])\n",
    "        d['dtc'] = np.linalg.norm(d[['Latitude', 'Longitude']] - self.city_center, axis=1)\n",
    "        super().fit(d)\n",
    "        self.enc.fit(d[self.cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        d = data.copy()\n",
    "        if self.gaps is not None:\n",
    "            for g in self.gaps:\n",
    "                d[g] = replace_zero(d[g])\n",
    "        d['dtc'] = np.linalg.norm(d[['Latitude', 'Longitude']] - self.city_center, axis=1)\n",
    "        cont = np.array(super().transform(d))\n",
    "        cat = np.array(self.enc.transform(d[self.cols]).todense())\n",
    "        return np.hstack((cont, cat))\n",
    "\n",
    "\n",
    "def make_ultimate_pipeline():\n",
    "\n",
    "    continuous_columns = ['Lot_Frontage', 'Lot_Area', 'Year_Built', 'Year_Remod_Add', \n",
    "                          'Mas_Vnr_Area', 'BsmtFin_SF_1', 'BsmtFin_SF_2', 'Bsmt_Unf_SF',\n",
    "                          'Total_Bsmt_SF', 'First_Flr_SF', 'Second_Flr_SF',\n",
    "                          'Gr_Liv_Area', 'Bedroom_AbvGr', 'TotRms_AbvGrd',\n",
    "                          'Garage_Cars', 'Garage_Area', 'Wood_Deck_SF',\n",
    "                          'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch',\n",
    "                          'Pool_Area', 'Mo_Sold', 'Year_Sold','dtc']\n",
    "    \n",
    "    categorical_column = ['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour',\n",
    "                      'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1',\n",
    "                      'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Qual', 'Overall_Cond',\n",
    "                      'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type',\n",
    "                      'Exter_Qual', 'Exter_Cond', 'Foundation', 'Bsmt_Qual', 'Bsmt_Cond',\n",
    "                      'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating',\n",
    "                      'Heating_QC', 'Central_Air', 'Electrical', 'Kitchen_Qual',\n",
    "                      'Functional', 'Fireplace_Qu', 'Garage_Type', 'Garage_Finish',\n",
    "                      'Garage_Qual', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence',\n",
    "                      'Misc_Feature', 'Sale_Type', 'Sale_Condition']\n",
    "    \n",
    "    interesting_columns = ['Fence', 'MS_SubClass', 'Utilities', 'Sale_Type',\n",
    "                           \"Overall_Qual\", \"Garage_Qual\", \"Sale_Condition\", \"MS_Zoning\",\n",
    "                           'Bsmt_Half_Bath', 'Half_Bath', 'Bsmt_Full_Bath',\n",
    "                           'Kitchen_AbvGr', 'Fireplaces', 'Full_Bath']\n",
    "    \n",
    "    interesting_columns = list(set(categorical_columns + interesting_columns))\n",
    "    oh_proc = OneHotPreprocessor(needed_columns=continuous_columns, cat_cols=interesting_columns,\n",
    "                                 gaps=None)\n",
    "    return Pipeline(steps=[('scaler', oh_proc), ('reg', Rigde())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "target_column = \"Sale_Price\"\n",
    "np.random.seed(seed)\n",
    "\n",
    "test_size = 0.2\n",
    "data_train, data_test, Y_train, Y_test = train_test_split(\n",
    "    data[data.columns.drop(\"Sale_Price\")],\n",
    "    np.array(data[\"Sale_Price\"]),\n",
    "    test_size=test_size,\n",
    "    random_state=seed)\n",
    "\n",
    "preprocessor = OneHotPreprocessor(\n",
    "    needed_columns=continuous_columns, interesting_columns=interesting_columns)\n",
    "\n",
    "X_train = preprocessor.fit_transform(data_train)\n",
    "X_test = preprocessor.transform(data_test)\n",
    "\n",
    "\n",
    "model = SGDLinearRegressor()\n",
    "\n",
    "\n",
    "def make_ultimate_pipeline():\n",
    "    pipe = Pipeline([('preprocessor', StandardScaler()), ('model', model)])\n",
    "    print(pipe.fit(X_train, Y_train).score(X_test, Y_test))\n",
    "\n",
    "\n",
    "make_ultimate_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaling\", MinMaxScaler()),\n",
    "        # the reduce_dim stage is populated by the param_grid\n",
    "        (\"reduce_dim\", \"passthrough\"),\n",
    "        (\"classify\", LinearSVC(dual=False, max_iter=10000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "N_FEATURES_OPTIONS = [2, 4, 8]\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"reduce_dim\": [PCA(iterated_power=7), NMF(max_iter=1_000)],\n",
    "        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n",
    "        \"classify__C\": C_OPTIONS,\n",
    "    },\n",
    "    {\n",
    "        \"reduce_dim\": [SelectKBest(mutual_info_classif)],\n",
    "        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n",
    "        \"classify__C\": C_OPTIONS,\n",
    "    },\n",
    "]\n",
    "reducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n",
    "\n",
    "grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_[\"mean_test_score\"])\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "# create a dataframe to ease plotting\n",
    "mean_scores = pd.DataFrame(\n",
    "    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n",
    ")\n",
    "\n",
    "ax = mean_scores.plot.bar()\n",
    "ax.set_title(\"Comparing feature reduction techniques\")\n",
    "ax.set_xlabel(\"Reduced number of features\")\n",
    "ax.set_ylabel(\"Digit classification accuracy\")\n",
    "ax.set_ylim((0, 1))\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimator applies a list of transformer objects in parallel to the input data,\n",
    "then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.\n",
    "During fitting, each of these is fit to the data independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a make_column_selector. The make_column_selector is used to select columns based on data type or column name:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"fare\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"embarked\", \"sex\", \"pclass\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "search_cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
